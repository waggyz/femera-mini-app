A Femera simulation is defined by a collection of models and methods.
Each model has a pointer to a local copy of its processing environment.
Each model also has a local data handler, which may be shared among sims.

Part : Partitioning model(s)
       Geom : Geometry model(s)
              Mesh : Finite element mesh
                     Elem : Element (FE) type(s), may be virtual or on-demand
                     Surf : Boundary element method (BEM) types
              Grid : Structured grids
                     Cell : FD, FV, Spec(tral) elements, etc.
              Gcad : Abstract geometry description
              Fine : Refinement methods
       Load : Boundary conditions (Boco?)
       Phys : Physics model(s)
              Mtrl : Material properties
       Cond : Preconditioner
       Solv : Solver(s)
       Sync : Synchronization method(s)
              Halo :
              Shft : Load balancing (Divy, Mete?, Bala? or Deal?)
       Data : Data I/O
              Plot : Visualization
              Save : Save or export
       Post : Post-processing task handler

Simulations, and their models, are decomposed into independent partitions. Each
partition is independent of other parts, except during synchronization. A sync
method, shared among several parts, handles data transfer among them. Partiitons
contain at least one sub-partition, mesh, or grid.

Sims->task: 1 or more sim runners (Frun)
            Frun->task: at least 1 Part and/or sub-sim runners (Frun)
                        Part->task: at least 1 Mesh, Grid, or child Part
                                    Mesh->task: may contain child Meshes

Each Mesh or Grid is homogeneous, containing only one type of element, one
material with its associated physics, one preconditioner, a set of boundary
conditions, one solution method, and a synchronizer.

Frun-> part [part_id] = <Geom*, Phys*, Cond*, Load*, Solv*, Sync*, Post*>


MPI+OpenMP simulation and partition distribution

Assign at least 1 MPI rank/NUMA domain.

Size  Levels  Sim level  Sim dist.  Part level  Part dist.  Data level
  XS  3*      2 (OpenMP) 1/OpenMP   2 (OpenMP)  ***         1 independent
  SM  3*      1 (MPI)    1/MPI      2 (OpenMP)  1/OpenMP    1 independent
  MD  3**     1 (team)   1/node^    2 (OpenMP)  1/OpenMP    1 collective team
  LG  3**     1 (team)   1/team^^   2 (OpenMP)  1/OpenMP    1 collective team
  XL  3*      0 (master) 1/all MPI  2 (OpenMP)  1/OpenMP    1 collective world

  * Master (0), MPI rank (1), OpenMP core (2)
 ** Master (0), MPI team (1), OpenMP core (2)
*** All parts for a sim are run in serial on 1 OpenMP core.
  ^ 1 MPI team/node
 ^^ Each MPI team spans several nodes.


Sim   Team size      MPI Team          OpenMP Team       Each Thread
Size  MPI    OMP     Obj. Concurrency  Obj. Concurrency  Obj. Conc.  Obj. Conc.
--------------------------------------------------------------------------------
  XS  --     1       --                Sims independent  Frun indp.  Part serial
  SM  1*     >1/MPI  Sims independent  Frun independent  Part coll.  Elem serial
  MD  1/node >1/MPI  Sims independent  Frun collective   Part coll.  Elem serial
  LG  n**    >1/MPI  Sims independent  Frun collective   Part coll.  Elem serial
  XL  world  >1/MPI  Sims serial       Frun collective   Part coll.  Elem serial

*   1 MPI/team, notionally, number of teams is number of MPI threads
**  n node/team, 1 < n < total nodes

Simulation and part scheduling

Plan::List fixed list sequential or round-robin
Plan::Fifo first in/first out queue
Plan::Auto automatic schedule selection based on problem size


